{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSP 556 Assignment_4\n",
    "### Sheikh-Sedat Touray\n",
    "# Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder as ohc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type Classification Problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the forest cover type data set as a data frame \n",
    "covert = pd.read_csv(\"covtype.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features and target variables \n",
    "x = covert.drop([\"Cover_Type\"], axis=1)\n",
    "y = covert[\"Cover_Type\"]\n",
    "y = covert[\"Cover_Type\"].astype(\"category\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am choosing 10000 observations and 25 columns \n",
    "x,y = make_classification(n_samples=10000, n_features=25, random_state=42)\n",
    "# SPlit the training and testing sets \n",
    "X_traint, X_testt, y_traint, y_testt = train_test_split(x, y, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_traint = sc.fit_transform(X_traint)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_testt = sc.transform(X_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 35 ms, total: 22.5 s\n",
      "Wall time: 23 s\n",
      "Grid Search: best parameters: {'max_depth': 6, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "#Initialize my random forest model \n",
    "Rforest = RandomForestClassifier(min_samples_split=2, random_state=0)\n",
    "# grid search\n",
    "param_grid = {'max_depth': list(range(1,7)), 'n_estimators': list(range(1,11)) }\n",
    "#Grid search with 5 fold CV on the Rforest model\n",
    "grid = GridSearchCV(Rforest, param_grid, cv=5)\n",
    "#fit the grid on the training set \n",
    "%time grid.fit(X_traint, y_traint)\n",
    "#Print out the best Parameters \n",
    "print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RForest Model: 0.92\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_model = grid.best_estimator_\n",
    "#Use our best model to predict on unseen data\n",
    "predictt_y = best_model.predict(X_testt)\n",
    "#Compute the accuracy score and Print it out\n",
    "acct = accuracy_score(y_testt, predictt_y)\n",
    "print(\"Accuracy of RForest Model: {:3.2f}\".format(acct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Random forest we trained our model for about 23 secs and had an accuracy score of 92.There's possible not that much of data leakage because the model is great at shuffling and randomizing the samples. However, the only leakages we might incur is in the scaling of the data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am choosing 5000 observations and 20 columns \n",
    "x,y = make_classification(n_samples=5000, n_features=20, random_state=42)\n",
    "# SPlit the training and testing sets \n",
    "X_trainb, X_testb, y_trainb, y_testb = train_test_split(x, y, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_trainb = sc.fit_transform(X_trainb)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_testb = sc.transform(X_testb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.2 s, sys: 18.2 s, total: 45.4 s\n",
      "Wall time: 31.5 s\n",
      "Grid Search: best parameters: {'base_estimator': DecisionTreeClassifier(), 'n_estimators': 9}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the bagging classifier \n",
    "bag = BaggingClassifier(random_state=3)\n",
    "# grid search\n",
    "param_gridb = {'base_estimator': [KNeighborsClassifier(n_neighbors=1),DecisionTreeClassifier()], 'n_estimators': list(range(1,11)) }\n",
    "#Perform gridsearch with 5-fold cross validation of the bag model \n",
    "grid1 = GridSearchCV(bag, param_gridb, cv=5)\n",
    "# fit the grid on the training set\n",
    "%time grid1.fit(X_trainb, y_trainb)\n",
    "#Print out the best parameters \n",
    "print(\"Grid Search: best parameters: {}\".format(grid1.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Bagging Classifier: 0.90\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_bag = grid1.best_estimator_\n",
    "#Use our best bag model to predict on unseen data\n",
    "predictb_y = best_bag.predict(X_testb)\n",
    "#Compute the accuracy score of our best model \n",
    "accb = accuracy_score(y_testb, predictb_y)\n",
    "#Print out the score \n",
    "print(\"Accuracy Score of Bagging Classifier: {:3.2f}\".format(accb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I used a sample size of 3000 it trained for 11 secs and gave an Accuracy of 98%. So I decided to use a larger sample size of 5000 observations and it trained for 27 secs with a accuracy of 90%. \n",
    "\n",
    "**Data leakage scenario:** Data leakage could happen If the base models in the ensemble are trained on the same dataset or if there is any overlap in the samples used to train different models, it can lead to data leakage.\n",
    "\n",
    "**My Prevention**: I split the data into test and train subsets before any preprocessing. I also did on use fit and fit_transform on the test subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am choosing 1000 observations and 20 columns \n",
    "x,y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "# SPlit the training and testing sets \n",
    "X_traina, X_testa, y_traina, y_testa = train_test_split(x, y, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_traina = sc.fit_transform(X_traina)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_testa = sc.transform(X_testa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.2 s, sys: 0 ns, total: 32.2 s\n",
      "Wall time: 33.6 s\n",
      "Grid Search: best parameters: {'base_estimator': DecisionTreeClassifier(random_state=3), 'n_estimators': 1}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Adaboost classifier \n",
    "boost = AdaBoostClassifier(random_state=3)\n",
    "# grid search\n",
    "param_grida = {'base_estimator': [SVC(probability=True, kernel='linear'),DecisionTreeClassifier(random_state=3)], 'n_estimators': list(range(1,11))}\n",
    "# Do a grid search on the adabosst with 5 fold CV\n",
    "grida = GridSearchCV(boost, param_grida, cv=5)\n",
    "#fit the grid on the training set and time it\n",
    "%time grida.fit(X_traina, y_traina)\n",
    "#Print out the best parameters and the timing\n",
    "print(\"Grid Search: best parameters: {}\".format(grida.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Adaboost: 0.83\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_boost = grida.best_estimator_\n",
    "# Use our best model to predict on test data \n",
    "predicta_y = best_boost.predict(X_testa)\n",
    "#Compute the accuracy score \n",
    "acca = accuracy_score(y_testa, predicta_y)\n",
    "#Print out the score \n",
    "print(\"Accuracy Score of Adaboost: {:3.2f}\".format(acca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had an Accuracy of 89% with a sample size of 2000, however, it 2 minutes to train. when I reduced the sample size to 1000 with the same size of features the training time was reduced to 32 secs, however, the accuracy score went down to 84%. \n",
    "\n",
    "**Data leakage Scenario:** In boosting, each base model corrects the errors of its predecessor. If there is information leakage between the models, where the next model learns from the mistakes of the previous ones on the same set of data, it can lead to poor generalization and overfitting.\n",
    "\n",
    "**Algorithm leakage Prevention:** The algorithm depends on re-weighting of misclassified samples. Also our careful splitting before processing and adhering to the scaling rule of not applying fit or fit_transform to the test subset could help prevent leakage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am choosing 8000 observations and 25 columns \n",
    "x,y = make_classification(n_samples=2000, n_features=20, random_state=42)\n",
    "# SPlit the training and testing sets \n",
    "X_trainga, X_testga, y_trainga, y_testga = train_test_split(x, y, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_trainga = sc.fit_transform(X_trainga)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_testga = sc.transform(X_testga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 s, sys: 0 ns, total: 29.3 s\n",
      "Wall time: 31.7 s\n",
      "Grid Search: best parameters: {'criterion': 'friedman_mse', 'max_depth': 3, 'n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier(random_state=0)\n",
    "# grid search\n",
    "param_gridga = {'criterion': ['friedman_mse','mse'], 'n_estimators': list(range(1,11)), 'max_depth': [1,3,5,7]}\n",
    "#5 fold CV with grid search \n",
    "gridga = GridSearchCV(gboost, param_gridga, cv=5)\n",
    "#Fit the grid on my training set and time it \n",
    "%time gridga.fit(X_trainga, y_trainga)\n",
    "#Print out my best parameters\n",
    "print(\"Grid Search: best parameters: {}\".format(gridga.best_params_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of GradientBoost: 0.90\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_gboost = gridga.best_estimator_\n",
    "#using our best model to predict on unseen data\n",
    "predictga_y = best_gboost.predict(X_testga)\n",
    "#Compute and print the accuracy score of the classification\n",
    "accga = accuracy_score(y_testga, predictga_y)\n",
    "print(\"Accuracy Score of GradientBoost: {:3.2f}\".format(accga))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the gradient boosting I got a 90% accuracy from training of about 29 secs and some of the best parameters include max_depth of 3 meaning the best tree had 3 terminal nodes, the number of base estmators in the final model is 5 and the best criterion is ‘friedman_mse’ and this was expected because it is generally the best as it can provide a better approximation in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am choosing 500 observations and 20 columns \n",
    "x,y = make_classification(n_samples=500, n_features=20, random_state=42)\n",
    "# SPlit the training and testing sets \n",
    "X_trainst, X_testst, y_trainst, y_testst = train_test_split(x, y, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_trainst = sc.fit_transform(X_trainst)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_testst = sc.transform(X_testst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimators = \n",
    "# Define base models\n",
    "base_modelc = [\n",
    "    ('forestc', RandomForestClassifier(random_state=0)),\n",
    "    ('gradbc', GradientBoostingClassifier(random_state=0))\n",
    "]\n",
    "\n",
    "# Define the final-model\n",
    "final_modelc = LogisticRegression()\n",
    "\n",
    "# Define the StackingRegressor\n",
    "stkc = StackingClassifier(estimators=base_modelc, final_estimator=final_modelc, cv=5)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_gridstkc = {\n",
    "    'forestc__n_estimators': [10, 20],\n",
    "    \n",
    "    'final_estimator__C': [0.1, 1.0, 4.0]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.2 s, sys: 0 ns, total: 55.2 s\n",
      "Wall time: 57.1 s\n",
      "Grid Search: best parameters: {'final_estimator__C': 4.0, 'forestc__n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "#Perform gridsearch on the stacking classifier with 5 fold CV\n",
    "gridstkc = GridSearchCV(stkc, param_gridstkc, cv=5)\n",
    "#fit the grid on the training set and time it \n",
    "%time gridstkc.fit(X_trainst, y_trainst)\n",
    "#Print out the best parameters \n",
    "print(\"Grid Search: best parameters: {}\".format(gridstkc.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Stacking Classifier: 0.93\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_stk = gridstkc.best_estimator_\n",
    "#Use our best model to predict on test data \n",
    "predictstk_y = best_stk.predict(X_testst)\n",
    "#Compute the Accuracy score \n",
    "accstk = accuracy_score(y_testst, predictstk_y)\n",
    "#Print the score \n",
    "print(\"Accuracy Score of Stacking Classifier: {:3.2f}\".format(accstk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here i used a smaller sample size because of the number of hyperparameters it was taking a long time to train hence I played with the tuning parameters also until I was able to capture enough of them (parameters) in the model and a somewhat bearable runtime of 55 secs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data Ensemble Regression Problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing more libraries from Sci-kit learn \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the housing data as a data frame \n",
    "housing = pd.read_csv('ames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MS_SubClass</th>\n",
       "      <th>MS_Zoning</th>\n",
       "      <th>Lot_Frontage</th>\n",
       "      <th>Lot_Area</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>Lot_Shape</th>\n",
       "      <th>Land_Contour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>Lot_Config</th>\n",
       "      <th>...</th>\n",
       "      <th>Fence</th>\n",
       "      <th>Misc_Feature</th>\n",
       "      <th>Misc_Val</th>\n",
       "      <th>Mo_Sold</th>\n",
       "      <th>Year_Sold</th>\n",
       "      <th>Sale_Type</th>\n",
       "      <th>Sale_Condition</th>\n",
       "      <th>Sale_Price</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>141</td>\n",
       "      <td>31770</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Slightly_Irregular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>No_Fence</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>215000</td>\n",
       "      <td>-93.619754</td>\n",
       "      <td>42.054035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_High_Density</td>\n",
       "      <td>80</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Minimum_Privacy</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>105000</td>\n",
       "      <td>-93.619756</td>\n",
       "      <td>42.053014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>81</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Slightly_Irregular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>No_Fence</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172000</td>\n",
       "      <td>-93.619387</td>\n",
       "      <td>42.052659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>93</td>\n",
       "      <td>11160</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>No_Fence</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>244000</td>\n",
       "      <td>-93.617320</td>\n",
       "      <td>42.051245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two_Story_1946_and_Newer</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>74</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Slightly_Irregular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Minimum_Privacy</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>189900</td>\n",
       "      <td>-93.638933</td>\n",
       "      <td>42.060899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           MS_SubClass                 MS_Zoning  \\\n",
       "0  One_Story_1946_and_Newer_All_Styles   Residential_Low_Density   \n",
       "1  One_Story_1946_and_Newer_All_Styles  Residential_High_Density   \n",
       "2  One_Story_1946_and_Newer_All_Styles   Residential_Low_Density   \n",
       "3  One_Story_1946_and_Newer_All_Styles   Residential_Low_Density   \n",
       "4             Two_Story_1946_and_Newer   Residential_Low_Density   \n",
       "\n",
       "   Lot_Frontage  Lot_Area Street            Alley           Lot_Shape  \\\n",
       "0           141     31770   Pave  No_Alley_Access  Slightly_Irregular   \n",
       "1            80     11622   Pave  No_Alley_Access             Regular   \n",
       "2            81     14267   Pave  No_Alley_Access  Slightly_Irregular   \n",
       "3            93     11160   Pave  No_Alley_Access             Regular   \n",
       "4            74     13830   Pave  No_Alley_Access  Slightly_Irregular   \n",
       "\n",
       "  Land_Contour Utilities Lot_Config  ...            Fence Misc_Feature  \\\n",
       "0          Lvl    AllPub     Corner  ...         No_Fence         None   \n",
       "1          Lvl    AllPub     Inside  ...  Minimum_Privacy         None   \n",
       "2          Lvl    AllPub     Corner  ...         No_Fence         Gar2   \n",
       "3          Lvl    AllPub     Corner  ...         No_Fence         None   \n",
       "4          Lvl    AllPub     Inside  ...  Minimum_Privacy         None   \n",
       "\n",
       "  Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price  Longitude  \\\n",
       "0        0       5      2010       WD          Normal     215000 -93.619754   \n",
       "1        0       6      2010       WD          Normal     105000 -93.619756   \n",
       "2    12500       6      2010       WD          Normal     172000 -93.619387   \n",
       "3        0       4      2010       WD          Normal     244000 -93.617320   \n",
       "4        0       3      2010       WD          Normal     189900 -93.638933   \n",
       "\n",
       "    Latitude  \n",
       "0  42.054035  \n",
       "1  42.053014  \n",
       "2  42.052659  \n",
       "3  42.051245  \n",
       "4  42.060899  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the first 5 rows and all the columns \n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2930 entries, 0 to 2929\n",
      "Data columns (total 35 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Lot_Frontage        2930 non-null   int64  \n",
      " 1   Lot_Area            2930 non-null   int64  \n",
      " 2   Year_Built          2930 non-null   int64  \n",
      " 3   Year_Remod_Add      2930 non-null   int64  \n",
      " 4   Mas_Vnr_Area        2930 non-null   int64  \n",
      " 5   BsmtFin_SF_1        2930 non-null   int64  \n",
      " 6   BsmtFin_SF_2        2930 non-null   int64  \n",
      " 7   Bsmt_Unf_SF         2930 non-null   int64  \n",
      " 8   Total_Bsmt_SF       2930 non-null   int64  \n",
      " 9   First_Flr_SF        2930 non-null   int64  \n",
      " 10  Second_Flr_SF       2930 non-null   int64  \n",
      " 11  Low_Qual_Fin_SF     2930 non-null   int64  \n",
      " 12  Gr_Liv_Area         2930 non-null   int64  \n",
      " 13  Bsmt_Full_Bath      2930 non-null   int64  \n",
      " 14  Bsmt_Half_Bath      2930 non-null   int64  \n",
      " 15  Full_Bath           2930 non-null   int64  \n",
      " 16  Half_Bath           2930 non-null   int64  \n",
      " 17  Bedroom_AbvGr       2930 non-null   int64  \n",
      " 18  Kitchen_AbvGr       2930 non-null   int64  \n",
      " 19  TotRms_AbvGrd       2930 non-null   int64  \n",
      " 20  Fireplaces          2930 non-null   int64  \n",
      " 21  Garage_Cars         2930 non-null   int64  \n",
      " 22  Garage_Area         2930 non-null   int64  \n",
      " 23  Wood_Deck_SF        2930 non-null   int64  \n",
      " 24  Open_Porch_SF       2930 non-null   int64  \n",
      " 25  Enclosed_Porch      2930 non-null   int64  \n",
      " 26  Three_season_porch  2930 non-null   int64  \n",
      " 27  Screen_Porch        2930 non-null   int64  \n",
      " 28  Pool_Area           2930 non-null   int64  \n",
      " 29  Misc_Val            2930 non-null   int64  \n",
      " 30  Mo_Sold             2930 non-null   int64  \n",
      " 31  Year_Sold           2930 non-null   int64  \n",
      " 32  Sale_Price          2930 non-null   int64  \n",
      " 33  Longitude           2930 non-null   float64\n",
      " 34  Latitude            2930 non-null   float64\n",
      "dtypes: float64(2), int64(33)\n",
      "memory usage: 801.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Selecting just the numerical data types to avoid encoding \n",
    "housing2 = housing.select_dtypes(include='number')\n",
    "#This is to see more of what is in the dataset \n",
    "housing2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data set into predictors and a response\n",
    "feat = housing2.drop(['Sale_Price'], axis = 1)\n",
    "targ = housing2['Sale_Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a subset of the data to avoid long run times so we random sample\n",
    "#But here I used all the features since it is not as much as the forest data\n",
    "feat,targ = make_regression(n_samples=2000, n_features=35, random_state=42)\n",
    "#split into 70% training and 30% testing \n",
    "X_trainh, X_testh, y_trainh, y_testh = train_test_split(feat,targ, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_trainh = sc.fit_transform(X_trainh)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_testh = sc.transform(X_testh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.8 s, sys: 0 ns, total: 24.8 s\n",
      "Wall time: 1min 15s\n",
      "Grid Search: best parameters: {'max_depth': 6, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# decision trees\n",
    "Rforest = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# grid search\n",
    "param_gridf = {'max_depth': list(range(1,7)), 'n_estimators': list(range(1,11)) }\n",
    "gridf = GridSearchCV(Rforest, param_gridf, cv=5)\n",
    "#time the output of the best parameters and what they are\n",
    "%time gridf.fit(X_trainh, y_trainh)\n",
    "#Print the best parameters \n",
    "print(\"Grid Search: best parameters: {}\".format(gridf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of RForest: 11830.80\n",
      "R2 Score of RForest: 0.71\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_modelfr = gridf.best_estimator_\n",
    "# use our best model to predict on test data\n",
    "predictfr_y = best_modelfr.predict(X_testh)\n",
    "#compute the mean squared error and r2  score \n",
    "msef = mean_squared_error(y_testh, predictfr_y)\n",
    "r2f = r2_score(y_testh, predictfr_y)\n",
    "#Print out the metrics \n",
    "print(\"MSE of RForest: {:3.2f}\".format(msef))\n",
    "print(\"R2 Score of RForest: {:3.2f}\".format(r2f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error measures the squared distance between what my predictor predicted and the actual value and by looking at our MSE value of 11830, it is not a bad prediction considering the price of the houses are in the hundred thousands. Our coefficient of determination of 0.71 is very good also because it how well the data fit my model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a subset of the data to avoid long run times so we random sample\n",
    "#But here I used all the features since it is not as much as the forest data\n",
    "feat,targ = make_regression(n_samples=2930, n_features=35, random_state=42)\n",
    "#split into 70% training and 30% testing \n",
    "X_trainbg, X_testbg, y_trainbg, y_testbg = train_test_split(feat,targ, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_trainbg = sc.fit_transform(X_trainbg)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_testbg = sc.transform(X_testbg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.1 s, sys: 597 ms, total: 43.7 s\n",
      "Wall time: 45 s\n",
      "Grid Search: best parameters: {'base_estimator__max_depth': None, 'max_features': 1.0, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "bagr = BaggingRegressor(base_estimator = DecisionTreeRegressor(),random_state=0)\n",
    "# grid search\n",
    "param_gridbr = {\n",
    "    'n_estimators': [5, 10, 20],\n",
    "    'max_features': [0.5, 0.7, 1.0],\n",
    "    'base_estimator__max_depth': [None, 5, 10]\n",
    "}\n",
    "#Perform grid search and 5-fold CV \n",
    "gridbr = GridSearchCV(bagr, param_gridbr, cv=5)\n",
    "#Fit the grid on training set and time it \n",
    "%time gridbr.fit(X_trainbg, y_trainbg)\n",
    "#Print out the best parameters and time \n",
    "print(\"Grid Search: best parameters: {}\".format(gridbr.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Bagging Regressor: 4045.64\n",
      "R2 Score of Bagging Regressor: 0.86\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_bagr = gridbr.best_estimator_\n",
    "#use our best model to predict on test data\n",
    "predictbr_y = best_bagr.predict(X_testbg)\n",
    "#Compute the metrics \n",
    "msebr = mean_squared_error(y_testbg, predictbr_y)\n",
    "r2br = r2_score(y_testbg, predictbr_y)\n",
    "#Print out the metrics \n",
    "print(\"MSE of Bagging Regressor: {:3.2f}\".format(msebr))\n",
    "print(\"R2 Score of Bagging Regressor: {:3.2f}\".format(r2br))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error measures the squared distance between what my predictor predicted and the actual value and by looking at our MSE value of 4046, it is not a bad prediction considering the price of the houses are in the hundred thousands. Our coefficient of determination of 0.86 is very good also because it how well the data fit my model. This is my best performing model so far on this dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a subset of the data to avoid long run times so we random sample\n",
    "#But here I used all the features since it is not as much as the forest data\n",
    "feat,targ = make_regression(n_samples=2000, n_features=35, random_state=0)\n",
    "#split into 70% training and 30% testing \n",
    "X_trainarg, X_testarg, y_trainarg, y_testarg = train_test_split(feat,targ, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_trainarg = sc.fit_transform(X_trainarg)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_testarg = sc.transform(X_testarg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 174 ms, total: 21 s\n",
      "Wall time: 21.9 s\n",
      "Grid Search: best parameters: {'learning_rate': 0.0001, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "rboost = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(),random_state=0)\n",
    "# grid search\n",
    "param_gridarg = {\n",
    "    'n_estimators': [5, 10, 20],\n",
    "    'learning_rate': [0.00001, 0.0001, 0.001]\n",
    "    \n",
    "}\n",
    "#Perform gridsearch and 5-fold CV\n",
    "gridada = GridSearchCV(rboost, param_gridarg, cv=5)\n",
    "#fit the grid on the training set and time it \n",
    "%time gridada.fit(X_trainarg, y_trainarg)\n",
    "#Print out the best parameters \n",
    "print(\"Grid Search: best parameters: {}\".format(gridada.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Adaboost: 5478.63\n",
      "R2 Score of Adaboost: 0.76\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_boostarg = gridada.best_estimator_\n",
    "#USe our best model to predict on test data\n",
    "predictarg_y = best_boostarg.predict(X_testarg)\n",
    "#Compute the metrics \n",
    "msearg = mean_squared_error(y_testarg, predictarg_y)\n",
    "r2arg = r2_score(y_testarg, predictarg_y)\n",
    "#Print out the metrics \n",
    "print(\"MSE of Adaboost: {:3.2f}\".format(msearg))\n",
    "print(\"R2 Score of Adaboost: {:3.2f}\".format(r2arg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performed better than the forest, however, it did not do as well as the Bagging Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a subset of the data to avoid long run times so we random sample\n",
    "#But here I used all the features since it is not as much as the forest data\n",
    "feat,targ = make_regression(n_samples=2000, n_features=35, random_state=42)\n",
    "#split into 70% training and 30% testing \n",
    "X_traingb, X_testgb, y_traingb, y_testgb = train_test_split(feat,targ, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_traingb = sc.fit_transform(X_traingb)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_testgb = sc.transform(X_testgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.9 s, sys: 0 ns, total: 47.9 s\n",
      "Wall time: 59 s\n",
      "Grid Search: best parameters: {'criterion': 'mse', 'max_depth': 7, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "rgboost = GradientBoostingRegressor(random_state=0)\n",
    "# Define the parameters\n",
    "param_gridgb = {'criterion': ['friedman_mse','mse'], 'n_estimators': list(range(1,11)), 'max_depth': [1,3,5,7]}\n",
    "# Grid search with 5-fold CV\n",
    "gridgb = GridSearchCV(rgboost, param_gridgb, cv=5)\n",
    "#fit the grid on the training set and time it \n",
    "%time gridgb.fit(X_traingb, y_traingb)\n",
    "#Print out the best parameters \n",
    "print(\"Grid Search: best parameters: {}\".format(gridgb.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of GradientBoost: 16075.36\n",
      "R2 of GradientBoost: 0.61\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_gboost = gridgb.best_estimator_\n",
    "#Use our best model to predict on test data\n",
    "predictgb_y = best_gboost.predict(X_testgb)\n",
    "#Compute the metrics \n",
    "accga = mean_squared_error(y_testgb, predictgb_y)\n",
    "r2ga = r2_score(y_testgb, predictgb_y)\n",
    "#Print out the metrics \n",
    "print(\"MSE of GradientBoost: {:3.2f}\".format(accga))\n",
    "print(\"R2 of GradientBoost: {:3.2f}\".format(r2ga))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensemble above did not perform as well as the previous models. However, the results are somewhat decent and could possibly be improved through further tuning of the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a subset of the data to avoid long run times so we random sample\n",
    "#But here I used all the features since it is not as much as the forest data\n",
    "feat,targ = make_regression(n_samples=500, n_features=35, random_state=42)\n",
    "#split into 70% training and 30% testing \n",
    "X_trainstr, X_teststr, y_trainstr, y_teststr = train_test_split(feat,targ, train_size=0.7, test_size=0.3, random_state=3)\n",
    "\n",
    "# Initialize the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_trainstr = sc.fit_transform(X_trainstr)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_teststr = sc.transform(X_teststr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimators = \n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('forest', RandomForestRegressor(random_state=42)),\n",
    "    ('gradb', GradientBoostingRegressor(random_state=42))\n",
    "]\n",
    "\n",
    "# Define the meta-model\n",
    "final_model = LinearRegression()\n",
    "\n",
    "# Define the StackingRegressor\n",
    "stkr = StackingRegressor(estimators=base_models, final_estimator=final_model, cv=5)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_gridstkr = {\n",
    "    'forest__n_estimators': [10,20],\n",
    "    'final_estimator__normalize': [True, False]\n",
    "}\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.2 s, sys: 0 ns, total: 51.2 s\n",
      "Wall time: 53.7 s\n",
      "Grid Search: best parameters: {'final_estimator__normalize': True, 'forest__n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "#Grid Search and 5-fold CV\n",
    "gridstkr = GridSearchCV(stkr, param_gridstkr, cv=5)\n",
    "#Fit the grid on training set and time it\n",
    "%time gridstkr.fit(X_trainstr, y_trainstr)\n",
    "#Print out the best parameters \n",
    "print(\"Grid Search: best parameters: {}\".format(gridstkr.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Stacking Regressor: 5407.31\n",
      "R2 of Stacking Regressor: 0.85\n"
     ]
    }
   ],
   "source": [
    "# accuracy of best model\n",
    "best_stkr = gridstkr.best_estimator_\n",
    "#Use our best model to predict on test data\n",
    "predictstkr_y = best_stkr.predict(X_teststr)\n",
    "#Compute metrics \n",
    "msestkr = mean_squared_error(y_teststr, predictstkr_y)\n",
    "r2stkr = r2_score(y_teststr, predictstkr_y)\n",
    "#Print out the metrics \n",
    "print(\"MSE of Stacking Regressor: {:3.2f}\".format(msestkr))\n",
    "print(\"R2 of Stacking Regressor: {:3.2f}\".format(r2stkr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacking regressor is my best ensemble considering we sampled only 1/6 of the data and it almost performed as well as the ensemble in which we used the entire dataset. The only reason I could not sample a larger size was due to the amount of time it took to train the samples was very long when a larger sample size was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:** I was split the data into training and testing sets before any processing steps to prevent data leakage. Also, I am very cautious not to apply fit or fit_transform to the test set. Also any information derived from entire data including the test set is calculated solely based on the training data. And FInally, When using cross-validation, make sure that each fold's training set is used exclusively for training, and the validation set is kept separate.\n",
    "\n",
    "- Also outwardly looking at our ensembles and results we can observe that there is hardly any bias in the models because there are no overfittings and underfittings and the variances are not very bad and could perhaps be reduced by sampling more data and fine tuning the parameters.Maybe this is so because mixture models and ensemble learning mitigates this issue of variance bias tradeoff. Because boosting combines many \"Weak\" (high bias) models in an ensemble to collectively lower the bias of the individual models while bagging combines \"Strong\" learners in a way that reduces variance like we have seen in my bagging regressor.  \n",
    "One must also be cautious not to overly fine tune parameters to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
